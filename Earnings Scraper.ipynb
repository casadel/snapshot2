{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal here is to write a function snapshot() that will obtain a company's quarterly earnings report from its investor relations website and output the relative metrics with which we are concerned (a \"snapshot\" of the report). In many cases, these metrics will consist of a company's current quarter earnings per share (EPS), current quarter revenue, and estimates for what these metrics will be in the next quarter, known as the \"guidance\". For many companies, however, there are various other metrics that concern us in addition to these, or in some cases instead of. Furthermore, with all the companies that report their quarterly earnings on their IR websites, there exists very little uniformity in the way in which their reports are structured. Thus, we have our work cut out for us.\n",
    "\n",
    "To start, we will try to parse the release of Netflix (NFLX). We are primarily concerned with identifying GAAP EPS and revenue along with guidance for these metrics for next quarter. For NFLX, we are also concerned with identifying net streaming adds.\n",
    "\n",
    "First task is to obtain the reports from the websites. In practice, we will want to have to program running maybe one minute before the expected earnings report time so that it is refreshing the page every tenth of a second or so and can have the report text the second it is released by the website. Reports are usually released as PDFs, although for NVDA they report in a press release in HTML format so we may have to account for this possibility.\n",
    "\n",
    "Most companies structure their reports such that there it consists of dialogue talking about the metrics followed by a table of comprehensive metrics and numbers. Will probably want to pull separate the two so they are individually parsable\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO:\n",
    "    \n",
    "    -Write the get pdf functions that will refresh on the quarterly results IR page and download the file\n",
    "        - get_nflx\n",
    "        - get_amzn\n",
    "        - get_twtr\n",
    "        - get_tsla\n",
    "        - get_aapl\n",
    "        \n",
    "    -Write the table parsers for each company that will get the information we want for each company from the table and get paragraphs containing keywords\n",
    "        - nflx_parser\n",
    "        - amzn_parser\n",
    "        - twtr_parser\n",
    "        - tsla_parser\n",
    "        - aapl_parser\n",
    "        \n",
    "    - Wrap up the notebook so that it is usable from command line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import itertools\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import matplotlib as mpl\n",
    "import matplotlib.cm as cm\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "pd.set_option('display.width', 500)\n",
    "pd.set_option('display.max_columns', 100)\n",
    "pd.set_option('display.notebook_repr_html', True)\n",
    "import seaborn as sns\n",
    "sns.set_style(\"whitegrid\")\n",
    "sns.set_context(\"poster\")\n",
    "\n",
    "import math\n",
    "import requests\n",
    "import time\n",
    "import json\n",
    "from bs4 import BeautifulSoup\n",
    "import sys\n",
    "\n",
    "import pdfminer\n",
    "from pdfminer.pdfinterp import PDFResourceManager, PDFPageInterpreter\n",
    "from pdfminer.converter import TextConverter\n",
    "from pdfminer.layout import LAParams\n",
    "from pdfminer.pdfpage import PDFPage\n",
    "from cStringIO import StringIO\n",
    "\n",
    "from pdfminer.pdfparser import PDFParser\n",
    "from pdfminer.pdfdocument import PDFDocument\n",
    "from pdfminer.pdfpage import PDFTextExtractionNotAllowed\n",
    "from pdfminer.converter import PDFPageAggregator\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Get the PDFs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_nflx(link_dict):\n",
    "    while True:\n",
    "        page = requests.get(link_dict['NFLX'])\n",
    "        soup = BeautifulSoup(page.text, 'html.parser')\n",
    "        q3_html = soup.find_all('div', {'class': 'accBody'})[0]\n",
    "        docs = q3_html.find_all('a')\n",
    "        dwnload = []\n",
    "        found = False\n",
    "        for doc in docs:\n",
    "            if doc.text == 'Q316 Letter to shareholders':\n",
    "                link = doc['href']\n",
    "                found = True\n",
    "                break\n",
    "        if found:\n",
    "            break\n",
    "        time.sleep(1)\n",
    "    link = 'https://ir.netflix.com/' + link\n",
    "    pdfile = requests.get(link)\n",
    "    with open('nflx.pdf', 'wb') as f:\n",
    "        f.write(pdfile.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_amzn(link_dict):\n",
    "    while True:\n",
    "        page = requests.get(link_dict['AMZN'])\n",
    "        soup = BeautifulSoup(page.text, 'html.parser')\n",
    "        q3_html = soup.find_all('div', {'class': 'a-section article-copy'})[0]\n",
    "        docs = q3_html.find_all('a')\n",
    "        dwnload = []\n",
    "        found = False\n",
    "        for doc in docs:\n",
    "            if doc.text == 'Q3 2016 Financial Results':\n",
    "                link = doc['href']\n",
    "                found = True\n",
    "                break\n",
    "        if found:\n",
    "            break\n",
    "        time.sleep(1)\n",
    "    pdfile = requests.get(link)\n",
    "    with open('amzn.pdf', 'wb') as f:\n",
    "        f.write(pdfile.content)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Code to parse the PDFs, extract tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def extract_layout_by_page(pdf_path):\n",
    "    \"\"\"\n",
    "    Extracts LTPage objects from a pdf file.\n",
    "    \n",
    "    slightly modified from\n",
    "    https://euske.github.io/pdfminer/programming.html\n",
    "    \"\"\"\n",
    "    laparams = LAParams()\n",
    "\n",
    "    fp = open(pdf_path, 'rb')\n",
    "    parser = PDFParser(fp)\n",
    "    document = PDFDocument(parser)\n",
    "\n",
    "    if not document.is_extractable:\n",
    "        raise PDFTextExtractionNotAllowed\n",
    "\n",
    "    rsrcmgr = PDFResourceManager()\n",
    "    device = PDFPageAggregator(rsrcmgr, laparams=laparams)\n",
    "    interpreter = PDFPageInterpreter(rsrcmgr, device)\n",
    "\n",
    "    layouts = []\n",
    "    for page in PDFPage.create_pages(document):\n",
    "        interpreter.process_page(page)\n",
    "        layouts.append(device.get_result())\n",
    "\n",
    "    return layouts\n",
    "\n",
    "TEXT_ELEMENTS = [\n",
    "    pdfminer.layout.LTTextBox,\n",
    "    pdfminer.layout.LTTextBoxHorizontal,\n",
    "    pdfminer.layout.LTTextLine,\n",
    "    pdfminer.layout.LTTextLineHorizontal\n",
    "]\n",
    "\n",
    "def flatten(lst):\n",
    "    \"\"\"Flattens a list of lists\"\"\"\n",
    "    return [subelem for elem in lst for subelem in elem]\n",
    "\n",
    "\n",
    "def extract_characters(element):\n",
    "    \"\"\"\n",
    "    Recursively extracts individual characters from \n",
    "    text elements. \n",
    "    \"\"\"\n",
    "    if isinstance(element, pdfminer.layout.LTChar):\n",
    "        return [element]\n",
    "\n",
    "    if any(isinstance(element, i) for i in TEXT_ELEMENTS):\n",
    "        return flatten([extract_characters(e) for e in element])\n",
    "\n",
    "    if isinstance(element, list):\n",
    "        return flatten([extract_characters(l) for l in element])\n",
    "\n",
    "    return []\n",
    "\n",
    "def does_it_intersect(x, (xmin, xmax)):\n",
    "    return (x <= xmax and x >= xmin)\n",
    "\n",
    "def convert_to_rows(characters):\n",
    "    x_limit = 10\n",
    "    y_limit = 5\n",
    "    paragraph_limit = 20\n",
    "\n",
    "    rows = []\n",
    "    row = []\n",
    "    cell = \"\"\n",
    "    prior_x = None\n",
    "    prior_y = None\n",
    "\n",
    "    y_s = [];\n",
    "    x_s = [];\n",
    "    for c in characters:\n",
    "        c_x, c_y = math.floor((c.bbox[0] + c.bbox[2]) / 2), math.floor((c.bbox[1] + c.bbox[3]) / 2)\n",
    "        if prior_x is not None and not (c_x - prior_x <= x_limit and abs(c_y - prior_y) <= y_limit):\n",
    "            if abs(c_y - prior_y) > y_limit:\n",
    "                row.append(cell)\n",
    "\n",
    "                # find the right row\n",
    "                for i in xrange(len(rows)):\n",
    "                    if abs(y_s[i] - prior_y) <= y_limit:\n",
    "                        for j in xrange(len(x_s[i])):\n",
    "                            if prior_x < x_s[i][j]:\n",
    "                                rows[i] = rows[i][:j] + row + rows[i][j:]\n",
    "                                x_s[i] = x_s[i][:j] + [prior_x] + x_s[i][j:]\n",
    "                                break\n",
    "                        else:\n",
    "                            rows[i] += row\n",
    "                            x_s[i].append(prior_x)\n",
    "                            break\n",
    "                        break\n",
    "                else:\n",
    "                    rows.append(row)\n",
    "                    y_s.append(prior_y)\n",
    "                    x_s.append([prior_x])\n",
    "\n",
    "                cell = \"\"\n",
    "                row = []\n",
    "            elif c_x - prior_x > x_limit:\n",
    "                row.append(cell)\n",
    "                cell = \"\"\n",
    "\n",
    "        cell += c.get_text()\n",
    "        prior_x = c_x\n",
    "        prior_y = c_y\n",
    "\n",
    "    # handle the last row\n",
    "    row.append(cell)\n",
    "    for i in xrange(len(rows)):\n",
    "        if abs(y_s[i] - prior_y) <= y_limit:\n",
    "            for j in xrange(len(x_s[i])):\n",
    "                if prior_x < x_s[i][j]:\n",
    "                    rows[i] = rows[i][:j] + row + rows[i][j:]\n",
    "                    x_s[i] = x_s[i][:j] + [prior_x] + x_s[i][j:]\n",
    "                    break\n",
    "            else:\n",
    "                rows[i] += row\n",
    "                x_s[i].append(prior_x)\n",
    "                break\n",
    "            break\n",
    "    else:\n",
    "        rows.append(row)\n",
    "        y_s.append(prior_y)\n",
    "        x_s.append([prior_x])\n",
    "        \n",
    "    # insert blank rows between particularly separated lines\n",
    "    for i in xrange(len(y_s) - 2, -1, -1):\n",
    "        if abs(y_s[i] - y_s[i+1]) > paragraph_limit:\n",
    "            rows = rows[:i+1] + [[]] + rows[i+1:]\n",
    "    \n",
    "    return rows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Page Parsers \n",
    "###Company specific\n",
    "\n",
    "functions:\n",
    "- parse_pages: takes the pdf files and converts them into analysable format\n",
    "- parser: company specific parser that retrieves info from document text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def parse_pages(url, parser):\n",
    "    \n",
    "    page_layouts = extract_layout_by_page(url)\n",
    "    #objects_on_page = set(type(o) for o in page_layouts[3])\n",
    "\n",
    "    pages = []\n",
    "    for i in xrange(len(page_layouts)):\n",
    "        current_page = page_layouts[i]\n",
    "\n",
    "        texts = []\n",
    "\n",
    "        # seperate text and rectangle elements\n",
    "        for e in current_page:\n",
    "            if isinstance(e, pdfminer.layout.LTTextBoxHorizontal):\n",
    "                texts.append(e)\n",
    "\n",
    "        # sort them into \n",
    "        characters = extract_characters(texts)\n",
    "        pages.append(convert_to_rows(characters))\n",
    "    parser(pages)           \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def nflx_parser(pages):\n",
    "    for page in pages:\n",
    "        if len(page) > 1 and len(page[1]) > 0 and \"Consolidated Statements of Operations \" == page[1][0]:\n",
    "            for row in page:\n",
    "                if len(row) > 0 and row[0] == \"Revenues\":\n",
    "                    print(\"Revenue: \" + row[2] + \",000\")\n",
    "                # we want the first Basic in the table\n",
    "                elif len(row) > 0 and row[0] == \"Basic\":\n",
    "                    print(\"Basic EPS: \" + row[2])\n",
    "                    break\n",
    "        if len(page) > 1 and len(page[0]) > 0 and \"Q3 Results\" in page[0][0]:\n",
    "            for idx in range(len(page)):\n",
    "                row = page[idx]\n",
    "                if not row:\n",
    "                    last_blank = idx\n",
    "                if len(row) > 0 and any(\"global net adds\" in s for s in row):\n",
    "                    paragraph = []\n",
    "                    nest_id = last_blank + 1\n",
    "                    while page[nest_id] != []:\n",
    "                        paragraph.append(page[nest_id])\n",
    "                        nest_id = nest_id+1\n",
    "                    paragraph = list(itertools.chain.from_iterable(paragraph))\n",
    "                    p = reduce((lambda x, y: x + y), paragraph)\n",
    "                    print p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def amzn_parser(pages):\n",
    "    page = pages[0]\n",
    "    tail = pages[1:]\n",
    "    flattened = list(itertools.chain.from_iterable(page))\n",
    "    if any(\"Consolidated Statements of Operations\" in s for s in flattened):\n",
    "        index1 = flattened.index('Total net sales ') + 1\n",
    "        print(\"Revenue: \" + flattened[index1] + \",000,000\") \n",
    "        index2 = flattened.index('Basic earnings per share ') + 2\n",
    "        print(\"Basic EPS: \" + flattened[index2])\n",
    "        amzn_parser(tail)\n",
    "    elif any(\"Segment Information\" in s for s in flattened):\n",
    "        index3 = flattened.index('Net sales ') + 2\n",
    "        print(\"AWS Rev: \" + flattened[index3] + ',000,000')\n",
    "    elif any(\"Fourth Quarter 2016 Guidance\" in s for s in flattened):\n",
    "        idx4 = flattened.index('Fourth Quarter 2016 Guidance ')\n",
    "        idx = idx4\n",
    "        paragraph = []\n",
    "        while flattened[idx] != ' ':\n",
    "            paragraph.append(flattened[idx])\n",
    "            idx = idx+1\n",
    "        paragraph = list(itertools.chain.from_iterable(paragraph))\n",
    "        p = reduce((lambda x, y: x + y), paragraph)\n",
    "        print p\n",
    "        amzn_parser(tail)\n",
    "    else:\n",
    "        if tail != []:\n",
    "            amzn_parser(tail)\n",
    "        else:\n",
    "            print \"No Data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def twtr_parser(pages):\n",
    "    page = pages[0]\n",
    "    tail = pages[1:]\n",
    "    flattened = list(itertools.chain.from_iterable(page))\n",
    "    if any(\"Consolidated Statements of Operations\" in s for s in flattened):\n",
    "        index1 = flattened.index('Revenue ') + 1\n",
    "            if index1:\n",
    "                idx = index1 + 1\n",
    "                print(\"Revenue: \" + flattened[index1] + flattened[idx] + \",000\")\n",
    "        index2 = flattened.index('Basic and diluted ') + 1\n",
    "            if index2:\n",
    "                idx2 = index2 + 1\n",
    "                print(\"EPS: \" + flattened[index2] + flattened[idx2]) \n",
    "    elif any(\"MAUs\" in s for s in flattened):\n",
    "        index3 = \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Put It All Together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fourth Quarter 2016 Guidance •  Net sales are expected to be between $42.0 billion and $45.5 billion, or to grow between 17% and 27% compared with fourth quarter 2015. This guidance anticipates approximately 60 basis points of favorable impact from foreign exchange rates. •  Operating income is expected to be between $0 and $1.25 billion, compared with $1.1 billion in fourth quarter 2015. •  This guidance assumes, among other things, that no additional business acquisitions, investments, restructurings, or legal settlements are concluded. A conference call will be webcast live today at 2:30 p.m. PT/5:30 p.m. ET, and will be available for at least three months at www.amazon.com/ir. This call will contain forward-looking statements and other material information regarding the Company’s financial and operating results. These forward-looking statements are inherently difficult to predict. Actual results could differ materially for a variety of reasons, including, in addition to the factors discussed above, the amount that Amazon.com invests in new business opportunities and the timing of those investments, the mix of products and services sold to customers, the mix of net sales derived from products as compared with services, the extent to which we owe income taxes, competition, management of \n",
      "Revenue: 32,714   ,000,000\n",
      "Basic EPS: 0.53   $ \n",
      "AWS Rev: 18,874  ,000,000\n"
     ]
    }
   ],
   "source": [
    "tix = ['TWTR', 'TSLA', 'NFLX', 'AMZN', 'AAPL']\n",
    "links = ['https://investor.twitterinc.com/index.cfm', 'http://ir.tesla.com/', 'https://ir.netflix.com/results.cfm' ,'http://phx.corporate-ir.net/phoenix.zhtml?c=97664&p=irol-reportsOther', ]\n",
    "\n",
    "ir_dict = dict(zip(tix, links))\n",
    "        \n",
    "#get_nflx(ir_dict)\n",
    "#parse_pages('nflx.pdf', nflx_parser)\n",
    "\n",
    "get_amzn(ir_dict)\n",
    "parse_pages('amzn.pdf', amzn_parser)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[u'\\u2022 Q3 adjusted EBITDA of $181 million, up 28% year-over-year, ',\n",
       "  u'Monthly Active Users (MAU)'],\n",
       " [u'representing an adjusted EBITDA margin of 29%.'],\n",
       " [u'In Millions'],\n",
       " [],\n",
       " [u'\\u2022 Average monthly active users (MAUs) were 317 million for Q3, up 3% '],\n",
       " [u'year-over-year and compared to 313 million in the previous quarter.',\n",
       "  u'313',\n",
       "  u'317'],\n",
       " [u'\\u2022 Average U.S. MAUs were 67 million for Q3, up 1% year-over-',\n",
       "  u'66',\n",
       "  u'67'],\n",
       " [u'year and compared to 66 million in the previous quarter. '],\n",
       " [u'\\u2022 Average international MAUs were 250 million for Q3,  ',\n",
       "  u'247',\n",
       "  u'250'],\n",
       " [u'up 4% year-over-year and compared to 247 million in  '],\n",
       " [u'the previous quarter.        ', u'Q2\\u201916', u'Q3\\u201916'],\n",
       " [u'\\u2022 ', u'Mobile MAUs represented 83% of total MAUs.'],\n",
       " [u'\\u2022 Average daily active usage* (DAU) grew 7% year-over-year,  '],\n",
       " [u'an acceleration from 5% in Q2 and 3% in Q1.'],\n",
       " [],\n",
       " [u'We\\u2019re focused on driving value across three key areas of our service: ',\n",
       "  u'Y/Y Growth Rate'],\n",
       " [u'audience, content, and revenue. We believe these will have the biggest '],\n",
       " [u'impact on our ability to increase audience growth, engagement and '],\n",
       " [u'monetization. Let\\u2019s go through each in more detail.   '],\n",
       " [],\n",
       " [u'Daily Active Users* (DAU)'],\n",
       " [],\n",
       " [u'5%', u'7%'],\n",
       " [],\n",
       " [u'Q2\\u201916', u'Q3\\u201916'],\n",
       " [u'Audience'],\n",
       " [],\n",
       " [u'Refining our core service and improving safety are critical to growing our '],\n",
       " [u'audience. In Q3, we saw accelerating rates of growth on a year-over-year '],\n",
       " [u'basis for daily active usage, Tweet impressions and time spent on Twitter for '],\n",
       " [u'the second consecutive quarter. The increase was largely driven by product '],\n",
       " [u'improvements (including better relevance in the timeline and notifications) as '],\n",
       " [u'well as organic growth. '],\n",
       " [],\n",
       " [u'While our efforts are beginning to drive growth in audience and engagement, '],\n",
       " [u'we believe there is still significant opportunity ahead. To get a better sense '],\n",
       " [u'of the size of this opportunity: consider that each day there are millions of '],\n",
       " [u'people that come to Twitter to sign up for a new account or reactivate an '],\n",
       " [u'existing account that has not been active in the last 30 days.'],\n",
       " [],\n",
       " [u'To capture this large opportunity, and drive daily active usage across the '],\n",
       " [u'millions of people at the top of our funnel each day, we\\u2019re refining our core '],\n",
       " [u'service in four key areas: onboarding, the home timeline, notifications and '],\n",
       " [u'Tweeting. We\\u2019re rapidly iterating across these four areas and encouraged '],\n",
       " [u'by the direct benefit we\\u2019ve seen from recent product improvements that are '],\n",
       " [u'driving results in Q3, following the inflection in Q2.  '],\n",
       " [],\n",
       " [u'* Daily active users (DAUs) are Twitter users who logged in or were otherwise authenticated and accessed Twitter '],\n",
       " [u'through our website, mobile website or mobile applications on any given day. Average DAUs for a period represent the '],\n",
       " [u'average of the DAUs at the end of such period. In the past, Twitter has discussed DAUs and the ratio of monthly active '],\n",
       " [u'users (MAUs) to DAUs. In those instances, for comparability and consistency with MAUs, DAUs also included users '],\n",
       " [u'who accessed Twitter through our desktop applications and third-party properties.']]"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "page_layouts = extract_layout_by_page('twtr.pdf')\n",
    "#objects_on_page = set(type(o) for o in page_layouts[3])\n",
    "\n",
    "pages = []\n",
    "for i in xrange(len(page_layouts)):\n",
    "    current_page = page_layouts[i]\n",
    "\n",
    "    texts = []\n",
    "\n",
    "    # seperate text and rectangle elements\n",
    "    for e in current_page:\n",
    "        if isinstance(e, pdfminer.layout.LTTextBoxHorizontal):\n",
    "            texts.append(e)\n",
    "\n",
    "    # sort them into \n",
    "    characters = extract_characters(texts)\n",
    "    pages.append(convert_to_rows(characters))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "flattened = list(itertools.chain.from_iterable(pages[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-170-bba889e32c10>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-170-bba889e32c10>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    p = map((lambda x: x = \" \" if x = [] else x), pages[2])\u001b[0m\n\u001b[0m                         ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "p = map((lambda x: x = \" \" if x = [] else x), pages[2])\n",
    "p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
