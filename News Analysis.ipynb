{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#NEWSWATCH SUPERFILTERS\n",
    "\n",
    "Motivation: We hope to conduct a text analysis of breaking news headlines and associated reports pulled from newswatch and entered into a spreadsheet by our team. All the articles in our database resulted in tangible price movements in the associated stocks and were available immediately upon release through newswatch. We will be seeking to identify keywords, phrases, and article tags that show up across a variety of news headlines with the intention of using the results to tailor newswatch filters such that we can get the news and see it right away.\n",
    "\n",
    "Specifically, we will start by investigating headlines in several key areas:\n",
    "    - Biotech\n",
    "    - M/A\n",
    "    - Corporate Activity\n",
    "    - Enforcement Agency Activity\n",
    "    - Patent Law\n",
    "    \n",
    "And will seek to further categorize headlines within these groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import matplotlib as mpl\n",
    "import matplotlib.cm as cm\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "pd.set_option('display.width', 500)\n",
    "pd.set_option('display.max_columns', 100)\n",
    "pd.set_option('display.notebook_repr_html', True)\n",
    "import seaborn as sns\n",
    "sns.set_style(\"whitegrid\")\n",
    "sns.set_context(\"poster\")\n",
    "from collections import Counter\n",
    "import codecs\n",
    "\n",
    "import nltk\n",
    "from nltk.collocations import *\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import FreqDist\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "#nltk.download('stopwords')\n",
    "#nltk.download(\"genesis\")\n",
    "#nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Read in Headline Spreadsheet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_spreadsheet(sheet):\n",
    "    news_df = pd.read_csv(sheet)\n",
    "    ind = news_df.index\n",
    "\n",
    "    remove_list = ['press release: ', 'dj: press release: ', 'top-line', 'phase 1', 'phase 2', 'phase 3', 'dj', 'u.s.', '*dj']\n",
    "    replace_list = ['', '', 'topline', 'phase1', 'phase2', 'phase3', '', 'us', '']\n",
    "\n",
    "    for num in ind:\n",
    "        tagset = news_df.Tags[num]\n",
    "        tagset = tagset.split(', ')\n",
    "        taglistlist = []\n",
    "        for taglist in tagset:\n",
    "            taglist = taglist.split()\n",
    "            taglistlist.append(taglist)\n",
    "        news_df.loc[num, 'Tags'] = taglistlist\n",
    "        news_df.loc[num, 'Vendors'] = news_df.loc[num, 'Vendors'].split(', ')\n",
    "\n",
    "        headline = str(news_df.Headline[num])\n",
    "        text = str(news_df.Text[num])\n",
    "        c_txt = str(news_df['Clean Text'][num])\n",
    "\n",
    "        news_df.loc[num, 'Headline'] = headline.lower()\n",
    "        news_df.loc[num, 'Text'] = text.lower()\n",
    "        news_df.loc[num, 'Clean Text'] = c_txt.lower()\n",
    "\n",
    "        for rem, rep in zip(remove_list, replace_list):\n",
    "            headline = news_df.Headline[num]\n",
    "            text = news_df.Text[num]\n",
    "            c_txt = news_df['Clean Text'][num]\n",
    "            news_df.loc[num, 'Headline'] = headline.replace(rem, rep)\n",
    "            news_df.loc[num, 'Text'] = text.replace(rem, rep)\n",
    "            news_df.loc[num, 'Clean Text'] = c_txt.replace(rem, rep)\n",
    "\n",
    "    return news_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Subset the dataframe and analyze tags, headlines, text\n",
    "\n",
    "###Most common tags by group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def subset_df(head_type, sub_type = None):\n",
    "    sub_df = news_df[news_df['Headline Type']== head_type]\n",
    "    if sub_type == None:\n",
    "        return sub_df\n",
    "    elif sub_type != None:\n",
    "        subsub = sub_df[(sub_df['Headline Sub Type']== sub_type)]\n",
    "        return subsub\n",
    "\n",
    "def tag_report(df, source):\n",
    "    vendset = df.Vendors\n",
    "    tagset = df.Tags\n",
    "    n = vendset.index\n",
    "    vend_ind = []\n",
    "    bigtaglist = []\n",
    "    #identify articles from given source, retrieve associated tags\n",
    "    for num in n:\n",
    "        vendlist = vendset[num]\n",
    "        for vend_num in range(0, len(vendlist)):\n",
    "            vendor = vendlist[vend_num]\n",
    "            if source in vendor:\n",
    "                vend_ind = vend_num\n",
    "                bigtaglist.append(tagset[num][vend_ind])\n",
    "    #no articles from source\n",
    "    if not bigtaglist:\n",
    "        return 'No articles from %s!!' % source\n",
    "    #aggregate tags and return count of most common\n",
    "    elif bigtaglist:\n",
    "        tag_list = []\n",
    "        for group in bigtaglist:\n",
    "            for tag in group:\n",
    "                tag_list.append(tag)\n",
    "\n",
    "        top_tags = Counter(tag_list)\n",
    "        return top_tags.most_common(35)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Bigram & Trigram Collocations, Keywords by Group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def make_soup(df):\n",
    "    txt_list = df['Clean Text']\n",
    "    heads_list = df.Headline\n",
    "    with open('txtsoup.txt', 'w') as txts:\n",
    "        for line in txt_list:\n",
    "            txts.write(\"%s\\n\" % line)\n",
    "    with open('headsoup.txt', 'w') as hds:\n",
    "        for line in heads_list:\n",
    "            hds.write(\"%s\\n\" % line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def keywords(soup):\n",
    "    fp = open(soup, 'r')\n",
    "    words = fp.read()\n",
    "    words = nltk.tokenize.word_tokenize(words)\n",
    "    stops = nltk.corpus.stopwords.words('english')\n",
    "    words = [word for word in words if len(word) > 3]\n",
    "    words = [word.lower() for word in words]\n",
    "    words = [w for w in words if w not in stops]\n",
    "    fdist = FreqDist(words)\n",
    "    return fdist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_bigrams(soup, n):\n",
    "    bigram_measures = nltk.collocations.BigramAssocMeasures()\n",
    "    # find collocations\n",
    "    bi_finder = BigramCollocationFinder.from_words(nltk.corpus.genesis.words(soup))\n",
    "    # only bigrams that appear n+ times\n",
    "    bi_finder.apply_freq_filter(n)\n",
    "    ignored_words = nltk.corpus.stopwords.words('english')\n",
    "    #filter stopwords\n",
    "    bi_finder.apply_word_filter(lambda w: len(w) < 3 or w.lower() in ignored_words)\n",
    "    bigrams = bi_finder.nbest(bigram_measures.pmi, 50)\n",
    "    return bigrams\n",
    "\n",
    "def get_trigrams(soup, n):\n",
    "    trigram_measures = nltk.collocations.TrigramAssocMeasures()\n",
    "    tri_finder = TrigramCollocationFinder.from_words(nltk.corpus.genesis.words(soup))\n",
    "    tri_finder.apply_freq_filter(n)\n",
    "    ignored_words = nltk.corpus.stopwords.words('english')\n",
    "    tri_finder.apply_word_filter(lambda w: len(w) < 3 or w.lower() in ignored_words)\n",
    "    trigrams = tri_finder.nbest(trigram_measures.pmi, 50)\n",
    "    return trigrams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('S.FO', 3),\n",
       " ('PERI.FLY', 2),\n",
       " ('MYL', 2),\n",
       " ('IPXL', 1),\n",
       " ('HOTS.FLY', 1),\n",
       " ('TEVA', 1),\n",
       " ('BAH', 1),\n",
       " ('LCI', 1)]"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news_df = get_spreadsheet('HeadlineSpreadsheet.csv')\n",
    "subsub = subset_df('Enforcement Agencies', )\n",
    "tag_report(subsub, 'Fly on the Wall')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FreqDist({'bank': 32, 'said': 23, 'company': 18, 'investigation': 17, 'deutsche': 15, 'billion': 15, 'people': 13, 'lemelson': 12, 'merger': 11, 'settlement': 10, ...})"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "make_soup(subsub)\n",
    "head_soup = '/Users/titans_bball30/Desktop/Trlmprojects/headsoup.txt'\n",
    "text_soup = '/Users/titans_bball30/Desktop/Trlmprojects/txtsoup.txt'\n",
    "keywords(text_soup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('new', 'york'),\n",
       " ('waiting', 'period'),\n",
       " ('://', 'www'),\n",
       " ('evaluate', 'whether'),\n",
       " ('senate', 'special'),\n",
       " ('alleged', 'abuses'),\n",
       " ('second', 'request'),\n",
       " ('business', 'practices'),\n",
       " ('capital', 'one'),\n",
       " ('special', 'committee'),\n",
       " ('orphan', 'drug'),\n",
       " ('purchase', 'agreement'),\n",
       " ('people', 'familiar'),\n",
       " ('justice', 'department'),\n",
       " ('drug', 'act'),\n",
       " ('nan', 'nan'),\n",
       " ('deutsche', 'bank'),\n",
       " ('people', 'said'),\n",
       " ('said', 'today'),\n",
       " ('lemelson', 'said')]"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_bigrams(text_soup, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('senate', 'special', 'committee'),\n",
       " ('orphan', 'drug', 'act'),\n",
       " ('lemelson', 'said', 'today')]"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_trigrams(text_soup, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(subsub.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
